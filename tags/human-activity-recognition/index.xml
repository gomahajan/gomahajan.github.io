<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Human Activity Recognition on Gaurav Mahajan</title>
    <link>https://gomahajan.github.io/tags/human-activity-recognition/</link>
    <description>Recent content in Human Activity Recognition on Gaurav Mahajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Gaurav Mahajan</copyright>
    <lastBuildDate>Sun, 20 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/human-activity-recognition/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Human Activity Recognition using Convolution Neural Network</title>
      <link>https://gomahajan.github.io/post/cnn-har-actitracker/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://gomahajan.github.io/post/cnn-har-actitracker/</guid>
      <description>&lt;p&gt;In this post&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, we will discuss how to build a architecture for Human Activity Recognition using Convolution Neural Network. &lt;a href=&#34;https://github.com/gomahajan/har-actitracker&#34;&gt;Link to code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;python-libraries-used&#34;&gt;Python Libraries Used&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;TensorFlow&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;Numpy&lt;/li&gt;
&lt;li&gt;Scipy&lt;/li&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;

&lt;p&gt;We are going to build a architecture which recognizes human activities like walking, lying down, standing, sitting, jogging etc. based on accelerometer data. We will use the Actitracker dataset from &lt;a href=&#34;http://www.cis.fordham.edu/wisdm/dataset.php&#34;&gt;Wireless Sensor Data Mining Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Data for some activities in the dataset:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://gomahajan.github.io/img/har-cnn-actitracker/walking.png&#34; alt=&#34;&#34;&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;img src=&#34;https://gomahajan.github.io/img/har-cnn-actitracker/lying-down.png&#34; alt=&#34;&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;

&lt;p&gt;Our architecture consists of multiple Depthwise Convolution Neural Networks (D-CONV), Max POOL and RELU layers, followed by fully connected neural network (FC) with a softmax classifier to get the class probabilities.&lt;/p&gt;

&lt;p&gt;[D-CONV -&amp;gt; RELU] -&amp;gt; POOL -&amp;gt; [D-CONV -&amp;gt; RELU] -&amp;gt; FC&lt;/p&gt;

&lt;p&gt;If you haven&#39;t heard of Depthwise Convolution Neural Networks, they are similar to normal Convolution Neural Networks except the filters are applied to each channel separately. So, a Convolution Neural Network which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, out_channels] will output a tensor of shape [batch, out_height, out_width, out_channels]. A Depthwise Convolution Neural Network which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] will output a tensor of shape [batch, out_height, out_width, in_channels*channel_multiplier] as it will separately apply channel_multiplier number of filters to each in_channel.&lt;/p&gt;

&lt;h1 id=&#34;building-the-architecture-in-tensorflow&#34;&gt;Building the architecture in TensorFlow&lt;/h1&gt;

&lt;p&gt;We create placeholder variables for our input and output values. In TensorFlow, using &#39;None&#39; as a dimension allows the variable to take any number of rows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, in_channels])
self.Y = tf.placeholder(tf.float32, shape=[None, num_labels])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we create a Depthwise Convolution Neural Network (D-CNN) which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] and will output a tensor of shape [batch, out_height, out_width, in_channels*channel_multiplier].&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filter = weight_variable([1, filter_width, in_channels, channels_multiplier])
depthwise_conv2d = tf.nn.depthwise_conv2d(input, filter, [1, 1, 1, 1], padding=&#39;VALID&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We follow the D-CNN with a RELU layer and add a bias for each channel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;biases = bias_variable([channels_multiplier * in_channels])
tf.nn.relu(tf.add(depthwise_conv2d, biases))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then add a max POOL layer which performs the max pooling over the window of ksize=[1, 1, kernel_size, 1] and uses a stride of [1, 1, stride_size, 1].
Note: We are performing the max pooling and stride only in the dimension of input_width. We do not want to pool across the channels or batch entries as we want to keep them independent. Also, input_height is 1 for D-CNN.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1],
                          strides=[1, 1, stride_size, 1], padding=&#39;VALID&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then add another D-CNN with filter tensor of shape [1, filter_width_2, in_channels_2, channel_multiplier_2]. in_channels_2 is equal to channels_multiplier * in_channels since D-CNN multiplies the number of channels by the channels_multiplier and our pooling preserved the
channel size.
Note: We, however, have to be careful when choosing the filter_width_2 as it should not be more than POOL layer&#39;s output width.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assert filter_width_2 &amp;lt;= p.shape[2], &amp;quot;Filter width 2 should be less than input width 2&amp;quot;
c = apply_depthwise_conv(p, filter_width_2, channels_multiplier * in_channels, channels_multiplier_2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We flatten the output from the second D-CNN and pass it into a neural network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shape = c.get_shape().as_list()
c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])

assert shape[3] == channels_multiplier * in_channels * channels_multiplier_2

f_weights_l1 = weight_variable([shape[1] * shape[2] * shape[3], num_hidden])
f_biases_l1 = bias_variable([num_hidden])
f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1), f_biases_l1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then add a softmax classifier which outputs the class probabilities and loss.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out_weights = weight_variable([num_hidden, num_labels])
out_biases = bias_variable([num_labels])
self.y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)
self.loss = -tf.reduce_sum(self.Y * tf.log(self.y_))
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;preprocessing-data&#34;&gt;Preprocessing data&lt;/h1&gt;

&lt;p&gt;Using pandas library, we can read the data from the WISDM dataset using column names as indices for the returned DataFrame.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;column_names = [&#39;user-id&#39;, &#39;activity&#39;, &#39;timestamp&#39;, &#39;x-axis&#39;, &#39;y-axis&#39;, &#39;z-axis&#39;]
data = pd.read_csv(file_path, header=None, names=column_names, comment=&#39;;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then drop the rows which have &#39;nan&#39; entries and reduce the size in case we want to use only a certain percentage of the dataset &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = data.dropna(axis=0, how=&#39;any&#39;)
data = data[0:(usage * data.shape[0]) // 100]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We normalize the data independently from accelerometer along the x-axis, y-axis and z-axis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normalize(dataset):
   mu = np.mean(dataset, axis=0)
   sigma = np.std(dataset, axis=0)
   return (dataset - mu) / sigma

dataset[&#39;x-axis&#39;] = math.normalize(dataset[&#39;x-axis&#39;])
dataset[&#39;y-axis&#39;] = math.normalize(dataset[&#39;y-axis&#39;])
dataset[&#39;z-axis&#39;] = math.normalize(dataset[&#39;z-axis&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we segment the data into [windows_size, in_channels] blocks and we choose the label as the most occurring label in the window.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def create_segments(data, window_size=90):
    segments = np.empty((0, window_size, 3))
    labels = np.empty((0))
    for (start, end) in windows(data[&#39;timestamp&#39;], window_size):
        x = data[&amp;quot;x-axis&amp;quot;][start:end]
        y = data[&amp;quot;y-axis&amp;quot;][start:end]
        z = data[&amp;quot;z-axis&amp;quot;][start:end]
        if (len(data[&#39;timestamp&#39;][start:end]) == window_size):
            segments = np.vstack([segments, np.dstack([x, y, z])])
            labels = np.append(labels, stats.mode(data[&amp;quot;activity&amp;quot;][start:end])[0][0])
    return segments, labels

segments, labels = util.create_segments(dataset)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then one hot encode the labels and reshape the segments into [1, windows_size, in_channels] blocks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)
reshaped_segments = segments.reshape(len(segments), 1, 90, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We do a 70:30 split of the data into train:test sets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_test_split = np.random.rand(len(reshaped_segments)) &amp;lt; 0.70
train_x = reshaped_segments[train_test_split]
train_y = labels[train_test_split]
test_x = reshaped_segments[~train_test_split]
test_y = labels[~train_test_split]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;training-and-evaluating-the-model&#34;&gt;Training and evaluating the model&lt;/h1&gt;

&lt;p&gt;We batch the training data into sets of 10 and use gradient descent for learning weights.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;learning_rate = 0.0001
training_epochs = 8
batch_size = 10
total_batches = train_x.shape[0] // batch_size
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)

correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

cost_history = np.empty(shape=[1], dtype=float)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We create a TensorFlow session, initialize the variables and train our model.
We print the accuracy on the training set for each epoch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with tf.Session() as session:
    tf.initialize_all_variables().run()
    for epoch in range(training_epochs):
        for b in range(total_batches):
            offset = (b * batch_size) % (train_y.shape[0] - batch_size)
            batch_x = train_x[offset:(offset + batch_size), :, :, :]
            batch_y = train_y[offset:(offset + batch_size), :]
            _, loss_value = session.run([optimizer, self.loss], feed_dict={self.X: batch_x, self.Y: batch_y})
            cost_history = np.append(cost_history, loss_value)
        print(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; Training Loss: &amp;quot;, loss_value, &amp;quot; Training Accuracy: &amp;quot;,
              session.run(accuracy, feed_dict={self.X: train_x, self.Y: train_y}))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then get our accuracy on the test set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Testing Accuracy:&amp;quot;, session.run(accuracy, feed_dict={self.X: test_x, self.Y: test_y}))
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Inspired from a similar &lt;a href=&#34;https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN&#34;&gt;post&lt;/a&gt; by Aaqib Saeed
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Helps when you are testing your architecture and do not want the &amp;quot;immenseness&amp;quot; of your data to slow you.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
