<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Proof on Gaurav Mahajan</title>
    <link>https://example.com/tags/proof/</link>
    <description>Recent content in Proof on Gaurav Mahajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Gaurav Mahajan</copyright>
    <lastBuildDate>Tue, 01 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/proof/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Finding Regression Function</title>
      <link>https://example.com/post/finding-regression-function/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/finding-regression-function/</guid>
      <description>&lt;p&gt;Let
\(X\in R^P\) denote a real valued random input vector, and \(Y\in R\) a real valued random output variable, with joint distribution \(Pr(X,Y)\).
We seek a function \(f(X)\) for predicting \(Y\) given values of the input \(X\). This theory requires a &lt;em&gt;loss function&lt;/em&gt; \(L(Y,f(X))\) for penalizing errors in prediction, and by far the most common and convenient is &lt;em&gt;squared error loss:&lt;/em&gt; \(L(Y,f(X))=(Y-f(X))^2\). This leads us to a criterion for choosing \(f\),&lt;/p&gt;

&lt;p&gt;\begin{align*}
EPE(f)&amp;amp; = E(Y-f(X))^2\\&lt;br&gt;
&amp;amp; = \int(y-f(x))^2 Pr(dx,dy)
\end{align*}&lt;/p&gt;

&lt;p&gt;the expected (squared) prediction error. By conditioning on \(X\), we can write \(EPE\) as &lt;span  class=&#34;math&#34;&gt;\(EPE(f) = E_X E_{Y|X} ((Y-f(X))^2|X)\)&lt;/span&gt;
and we see that it suffices to minimize \(EPE\) pointwise:
\begin{equation}
\label{epe}
f(x)=\underset{c}{\operatorname{argmin}} E_{Y|X} ([Y-c]^2|X=x)
\end{equation}&lt;/p&gt;

&lt;p&gt;For any guess t by writing \(\mu = E(Y)\), we have \(EPE(f)\) as
\begin{align*}
E[Y-t]^2 &amp;amp; = E[Y-\mu + \mu-t]^2\\&lt;br&gt;
&amp;amp; = E[Y-\mu]^2 + E[(Y-\mu)(\mu-t)] + E[\mu-t]^2\\&lt;br&gt;
&amp;amp; \geq E[Y-\mu]^2
\end{align*}&lt;/p&gt;

&lt;p&gt;Applied to conditional distribution of \(Y\) given \(X=x\), this gives us that &lt;span  class=&#34;math&#34;&gt;\( E_{Y\|X}([Y-E(Y\|X=x)]^2\|X=x) \leq E_{Y\|X} ([Y-c]^2\|X=x)\)&lt;/span&gt; for all estimates \(c\).&lt;/p&gt;

&lt;p&gt;Hence, proved that the solution for equation \eqref{epe} above is &lt;span  class=&#34;math&#34;&gt;\(f(x)=E(Y|X=x)\)&lt;/span&gt;
the conditional expectation, also known as the &lt;em&gt;regression&lt;/em&gt; function.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
