<!DOCTYPE html>
<html lang="en">

<head>
    <title>Gaurav Mahajan</title>
    <meta name="theme-color" content="#317EFB" />
    <meta name="description" content="PhD Student at UCSD theory group. Pursuing theoretical research in learning advised by Sanjoy Dasgupta and Shachar Lovett.">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<main>
    <div id="header">
        <img src="images/p2cropped.jpg" alt="Gaurav's photo" id="photo"></img>
        <br>
        <br>
        <br>
        <div class="bio">
            <h1 class="top-header">Gaurav Mahajan</h1>
            <p>
                I am a 4th year PhD student in the <a href="https://cstheory.ucsd.edu/home.html">theory group</a> at UCSD, where I am advised by <a href="https://cseweb.ucsd.edu/~dasgupta/">Sanjoy Dasgupta</a> and <a href="https://cseweb.ucsd.edu/~slovett/">Shachar Lovett</a>. Before that, I worked at <a href="https://powerapps.microsoft.com/en-us/">Microsoft</a> for three years. I received a BS and MS in Mathematics and Computing from Indian Institute of Technology, Delhi (IITD) advised by <a href="https://scholar.google.com/citations?hl=en&user=cs1R7C0AAAAJ">Subiman Kundu</a>.<br>
                <br>
                In Summer 2021, I am a research intern working with <a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a> and <a href="https://people.cs.umass.edu/~akshay/">Akshay Krishnamurthy</a> at Microsoft Research, New York. In Fall 2019, I was a short term scholar working with <a href="https://jasondlee88.github.io">Jason Lee</a> at <a href="https://www.math.ias.edu/sp/Optimization_Statistics_and_Theoretical_Machine_Learning"> Institute for Advanced Study</a>. In Summer 2019, I was a visiting graduate student working with <a href="https://jasondlee88.github.io">Jason Lee</a> at <a href="https://simons.berkeley.edu/programs/dl2019">Simons Institute</a>.<br>
            </p>
            <div class="backemail bottom-links">ude.dscu.gne (ta) najahamg  |  <a href="https://scholar.google.com/citations?user=3kvq284AAAAJ">[ralohcS elgooG]</a></div><br>
        </div>
    </div>
    <article>
        <p>
            I am broadly interested in questions related to learning and have recently worked on problems related to reinforcement learning,  active learning and unsupervised learning. 
        </p>
        <h1>Research<a href="#footnote">&ast;</a></h1> 
        <p>
            <b>Generalization in Reinforcement Learning:</b> Dealing with large state spaces is a major challenge in reinforcement learning. I am working on understanding what properties of Markov Decision Processes allow for algorithms to find optimal policies with sample complexity independent of size of state space.
        </p>
        <ol>
            <li>
                <p>
                    <b>Bilinear Classes: A Structural Framework for Provable Generalization in RL</b>
                    <br> <i> with Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Wen Sun and Ruosong Wang
                    <br> Accepted to the 38th International Conference on Machine Learning (ICML 2021). <font color="red">(Long Talk)</font></i>
                    <br> <a href="https://arxiv.org/abs/2103.10897">arxiv</a>
                </p>
                <p></p>
            </li>
            <li>
                <p>
                    <b>Q-learning with Function Approximation in Deterministic Systems: Tight Bounds on Approximation Error and Sample Complexity</b> 
                    <br> <i>with Simon Du, Jason Lee and Ruosong Wang</i>
                    <br> <i>Accepted to the 34th Annual Conference on Neural Information Processing Systems (NeurIPS 2020).</i>
                    <br> <a href="https://arxiv.org/abs/2002.07125">arxiv</a> | <a href="https://nips.cc/virtual/2020/protected/poster_fd5c905bcd8c3348ad1b35d7231ee2b1.html">talk</a>
                </p>
                <p></p>
            </li>
        </ol>
        <p>
            <b>Policy Gradient Methods:</b> Policy gradient (PG) methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. What can we say about their convergence properties: if and how fast do they converge to a globally optimal solution?
        </p>
        <ol>
            <li>
                <p>
                    <b>On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift</b>
                    <br> <i>with Alekh Agarwal, Sham Kakade and Jason Lee
                    <br> Accepted to the Journal of Machine Learning Research (JMLR 2021).</i>
                    <br> <a href="http://jmlr.org/papers/v22/19-736.html">jmlr</a> | <a href="https://arxiv.org/abs/1908.00261">arxiv</a>
                </p>
                <p></p>
            </li>
            <li>
                <p>
                    <b>Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes</b>
                    <br> <i>with Alekh Agarwal, Sham Kakade and Jason Lee
                    <br> Accepted to the 33rd Annual Conference on Learning Theory (COLT 2020).</i>
                    <br> <a href="http://proceedings.mlr.press/v125/agarwal20a.html">colt</a> | <a href="https://www.youtube.com/watch?v=ZGz57lbOK3w">talk</a> | <a href="https://www.youtube.com/watch?v=ZGz57lbOK3w">sham's talk</a>
                </p>
                <p></p>
            </li>
        </ol>
        <p>
             <b>Active Learning using Enriched Queries:</b> Active learning traditionally uses only label queries. I am interested in how asking better questions (comparison and membership queries) help with learning in this setting.
        </p>
        <ol>
            <li>
                <p>
                    <b>Point Location and Active Learning: Learning Halfspaces Almost Optimally</b>
                    <br> <i>with Max Hopkins, Daniel Kane And Shachar Lovett
                    <br> Accepted to the 61st Annual Symposium on Foundations of Computer Science (FOCS 2020).</i>
                    <br> <a href="https://arxiv.org/abs/2004.11380">arxiv</a> | <a href="https://www.youtube.com/watch?v=ETNMFJjBrpc">shachar's talk</a> | <a href="https://www.youtube.com/watch?v=FqtvnlmWyjI">max's talk</a> 
                </p>
                <p></p>
            </li>
            <li>
                <p>
                    <b>Noise-tolerant, Reliable Active Classification with Comparison Queries</b>
                    <br> <i>with Max Hopkins, Daniel Kane And Shachar Lovett
                    <br> Accepted to the 33rd Annual Conference on Learning Theory (COLT 2020).</i>
                    <br><a href="https://arxiv.org/abs/2001.05497">arxiv</a> | <a href="https://www.youtube.com/watch?v=gKLFBeaWBSs">max's talk</a>
                </p>
                <p></p>
            </li>
        </ol>

        <h1>Talks</h1>
        <ol>
            <li>
                Towards a Theory of Generalization in Reinforcement Learning<br>
                (UCSD Theory Seminar, UCSD Theory Lunch, ICML 2021)
            </li>
            <li>
                Policy Gradient Methods in Markov Decision Processes <br>
                (UCSD Theory Lunch, COLT 2020)
            </li>
        </ol>

        <h1>Professional Services</h1>
        <ol>
        <li>
            PC: WORLT (2021); Reviewer: Neurips (2021); ICML (2021, 2020), JMLR (2021, 2020)
        </li>
    </ol>
    <br>
    <br>
    <br>
    <p id="footnote"> &ast; Authors are listed in alphabetical order, following the convention in mathematics and theoretical computer science.</p>
    </article>
</main>

</html>