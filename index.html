<!DOCTYPE html>
<html lang="en">

<head>
    <title>Gaurav Mahajan</title>
    <meta name="theme-color" content="#444A60" />
    <meta name="description"
        content="PhD Student at UCSD theory group. Pursuing theoretical research in learning advised by Sanjoy Dasgupta and Shachar Lovett.">
    <link rel="stylesheet" type="text/css" href="css/style.css?v=2.1">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro' rel='stylesheet' type='text/css'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <main>
        <div class="dtable">
            <div class="half pad20" style="height: 60px;">
                <a href="./"><h1 class="top-header" style="font-weight: 500; margin: 0px;">Gaurav Mahajan</h1></a>
                <div class="hidden" style="text-align: left;"> <b style="font-weight: 700;">Email:</b>
                    gmahajan@eng.ucsd.edu</div><br>
            </div>
            <div class="hidden" style="float: left; padding: 15px 50px 0px 30px; width: 42%">
                <p style="text-align: right; margin: 5px;">
                    Office 4230,<br> CSE Building<br>
                    La Jolla, CA, 92092
                </p>
            </div>
        </div>
        <hr>
        <article>
            <div style="display: table; margin: 0px 0px 30px 0px;">
                <div class="half margin30" style="float: left;">
                    <img src="images/p2cropped.jpg" alt="Gaurav's photo" id="photo"></img>
                </div>
                <div class="half margin30" style="float: left;">
                    <p style="margin: 70px 0px 0px 0px;">
                        I am a 5th year PhD student in the <a href="https://cstheory.ucsd.edu/home.html">theory
                            group</a> at
                        UCSD advised by <a href="https://cseweb.ucsd.edu/~dasgupta/">Sanjoy Dasgupta</a> and
                        <a href="https://cseweb.ucsd.edu/~slovett/">Shachar Lovett</a>. <br> <br>
                        I have broad interests in problems related to learning algorithms.
                        Previously, I have spent some fun summers at Microsoft Research, Institute for Advanced Study
                        and Simons Institute.
                        <!-- In Summer 2021, I was a research intern working with <a
                            href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a> and <a
                            href="https://people.cs.umass.edu/~akshay/">Akshay Krishnamurthy</a> at Microsoft Research,
                        New
                        York. In Fall 2019, I was a short term scholar at <a
                            href="https://www.math.ias.edu/sp/Optimization_Statistics_and_Theoretical_Machine_Learning">
                            Institute for Advanced Study</a>. In Summer 2019, I was a visiting graduate student working
                        with
                        <a href="https://jasondlee88.github.io">Jason Lee</a> at <a
                            href="https://simons.berkeley.edu/programs/dl2019">Simons Institute</a>.  -->
                        <br><br> In the distant past, I worked at <a
                            href="https://powerapps.microsoft.com/en-us/">Microsoft</a> and received a
                        BS-MS from Indian Institute of Technology, Delhi (IITD).
                    </p>
                </div>
            </div>
            <div style="display: table;">
                <div class="half margin30" style="float: left;">
                    <h1>Upcoming/Recent Talks</h1>
                    <div class="flowlist" style="padding-bottom: 30px;">
                        <ol>
                            <li style="margin-top: 0.5em;
                            margin-bottom: 0.5em;">24 January 2023: MSR New York, on
                                Computational-Statistical Gaps in
                                RL.
                            </li>
                            <li style="margin-top: 0.5em;
                            margin-bottom: 0.5em;">13 January 2023: Yale Foundations of Data Science, on
                                Computational-Statistical Gaps in
                                RL.
                            </li>
                            <li style="margin-top: 0.5em;
                            margin-bottom: 0.5em;">19 December 2022: EnCORE Fall Retreat, on
                                Computational-Statistical Gaps in
                                RL.
                            </li>
                            <li style="margin-top: 0.5em;
                            margin-bottom: 0.5em;">8 June 2022: TTIC Machine Learning Seminar Series, on
                                Computational-Statistical Gaps in
                                RL.
                            </li>
                            <li style="margin-top: 0.5em;
                            margin-bottom: 0.5em;">29 Apr 2022: UCLA Big Data and Machine Learning weekly seminar, on
                                Computational-Statistical Gaps in
                                RL.
                            </li>
                            <li style="margin-top: 0.5em;
                        margin-bottom: 0.5em;">22 Apr 2022: Brown Robotics Group (<a
                                    href="https://cs.brown.edu/people/gdk/">George Konidaris</a>'s group), on
                                Computational-Statistical Gaps in
                                RL.
                            </li>
                            <li style="margin-top: 0.5em;
                        margin-bottom: 0.5em;">15 Apr 2022: Berkeley RL Reading Group (<a
                                    href="https://people.eecs.berkeley.edu/~jiantao/">Jiantao Jiao's</a> group), on
                                Computational-Statistical Gaps in
                                RL.
                            </li>
                            <li style="margin-top: 0.5em;
                        margin-bottom: 0.5em;">24 Jan 2022: <a
                                    href="https://www.cs.cornell.edu/content/realizable-learning-all-you-need">Cornell
                                    Theory Seminar</a>, on Equivalence between Realizable and Agnostic
                                Learning.
                            </li>
                            <li style="margin-top: 0.5em;
                        margin-bottom: 0.5em;">4 Oct 2021: <a href="https://cseweb.ucsd.edu//~dasgupta/259/">UCSD AI
                                    Seminar</a>, on Theory
                                of Generalization in Reinforcement Learning.</li>
                            <li style="margin-top: 0.5em;
                        margin-bottom: 0.5em;">14 Sep 2021: <a
                                    href="https://www.youtube.com/watch?v=OUJ-wP77VP0&ab_channel=RLtheoryseminars">RL
                                    Theory Seminar</a>, on Theory of Generalization in Reinforcement Learning. </li>
                            <li style="margin-top: 0.5em;
                        margin-bottom: 0.5em;">8 July 2021: MSR ML Reading Group, on Equivalence between Realizable and
                                Agnostic
                                Learning.
                            </li>
                            <li style="margin-top: 0.5em;
                        margin-bottom: 0.5em;">19 Apr 2021: <a
                                    href="https://cse.ucsd.edu/faculty-research/gaurav-mahajan-theory-seminar">UCSD
                                    Theory
                                    Seminar</a>, on Theory of Generalization in Reinforcement Learning.</li>
                        </ol>
                    </div>
                </div>
                <div class="half margin30" style="float: left;">
                    <h1>Research</h1>
                    <p style="margin: 20px 0px 0px 0px;">
                        I am interested in mathematical foundation of machine learning. I focus on designing provable
                        efficient algorithms for various learning problems (<a
                            href="https://arxiv.org/abs/2103.10897">RL with large state spaces</a>, hidden markov
                        models) and also generally intrigued by
                        global properties of simple local algorithms (<a href="https://arxiv.org/abs/1908.00261">policy
                            gradients</a>, <a href="https://arxiv.org/abs/2202.10640">k-means algorithm</a>, <a
                            href="https://arxiv.org/abs/2201.03806">multiplicative
                            weight update</a>). My current research areas include: 1) reinforcement learning theory and
                        2)
                        learning theory.
                        <br>
                        <br>
                        Currently, I am working on 1) computationally efficient algorithms for learning hidden markov
                        models,
                        2) computational lower bounds in RL with function approximation and 3) characterization for
                        learnability in PAC model with arbitrary distribution families.
                        <br>
                        <br>
                        See <a
                            href="https://scholar.google.com/citations?hl=en&user=3kvq284AAAAJ&view_op=list_works&sortby=pubdate">[Google
                            Scholar]</a>
                        and <a href="publications.html">here</a> for full list of publications.
                    </p>
                </div>
            </div>
            <div style="display: table;">
                <div class="margin30">
                    <h1>Selected Publications<a href="#footnote">&ast;</a></h1>
                    <ol>
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/2302.14753">Learning Hidden Markov Models Using Conditional Samples</a>
                                <br> with S.M. Kakade, A. Krishnamurthy, C. Zhang
                            </p>
                            <div class="abstract">
                                Hidden Markov Models are cryptographically hard to learn in the standard setting where one has access to i.i.d. samples of
                                observation sequences. We depart from this setup and consider an interactive access model,
                                in which the algorithm can query for samples from the conditional distributions of the HMMs. We
                                show that interactive access to the HMM enables computationally efficient learning algorithms, thereby
                                bypassing cryptographic hardness.
                            </div>
                            <p></p>
                        </li>
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/2202.05444">Computational-Statistical Gaps in
                                    Reinforcement Learning</a>
                                <br> with D. Kane, S. Lui, S. Lovett
                            </p>
                            <div class="abstract">
                                We resolve an open problem on designing computationally efficient algorithms for linear
                                RL setting (linear Q&ast;, linear V&ast;, linear Q&ast; &#38; V&ast;, &hellip;) by
                                showing that unless NP=RP, no polynomial time algorithm exists for any of these
                                settings. This is in contrast to the corresponding statistical problem, which was shown
                                to be solvable in polynomial sample complexity following a line of dedicated work from
                                the community. See this <a href="talks/ttic0607.mp4" target="_blank">
                                    <font color="red">talk</font>
                                </a> for a survey on this area.
                            </div>
                            <p></p>
                        </li>
                        <!-- <li>
                            <p>
                                <a href="https://arxiv.org/abs/2111.04746">Realizable Learning is All You Need</a>
                                <span class='options'> (COLT
                                    2022) </span>
                                    <br> with M. Hopkins, D. Kane, S. Lovett
                            </p>
                            <p></p>
                        </li>
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/2201.03806">Learning What To Remember</a>
                                <span class='options'> (ALT
                                    2022) </span>
                                    <br> with R. Bhattacharjee
                            </p>
                            <p></p>
                        </li> -->
                        <!-- <li>
                            <p>
                                <a href="https://arxiv.org/abs/2103.10897">Bilinear Classes: A Structural Framework for
                                    Provable Generalization in RL</a>

                                <span class='options'> (ICML
                                    2021) </span>
                                    <br>
                                        with S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, W. Sun, R.
                                        Wang

                            </p>
                            <p></p>
                        </li> -->
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/1908.00261">Optimality and Approximation with
                                    Policy
                                    Gradient Methods in Markov Decision
                                    Processes</a>
                                <br> with A. Agarwal, S. M. Kakade, J. D. Lee
                            </p>
                            <div class="abstract">
                                This work presents the first global convergence results for policy gradient methods like
                                vanilla policy gradient (w/wo regularization) and natural policy gradient. See this <a
                                    href="https://www.youtube.com/watch?v=tToomczBvMM&ab_channel=SimonsInstitute"
                                    target="_blank">
                                    <font color="red">talk</font>
                                </a> from Sham summarizing the results from this work.
                            </div>
                            <p></p>
                        </li>
                        <!-- <li>
                            <p>
                                <a href="https://arxiv.org/abs/2004.11380">Point Location and
                                    Active
                                    Learning: Learning Halfspaces Almost Optimally</a> <span class='options'> (FOCS
                                    2020) </span>
                                <br> <i>with M. Hopkins, D. Kane, S. Lovett
                                </i>
                            </p>
                            <p></p>
                        </li> -->

                    </ol>
                </div>
            </div>
            <!-- <div style="display: table;">
                <div class="margin30" style="float: left;">
                    <h1>New Preprints<a href="#footnote">&ast;</a></h1>
                    <p>
                    <ol>
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/2202.10640">Convergence of online k-means</a>
                                <br> <i>with S. Dasgupta, G. So
                                </i>
                                <br>
                                <a href="https://arxiv.org/abs/2202.10640">arxiv</a>
                            </p>
                            <p></p>
                        </li>
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/2202.05444">Computational-Statistical Gaps in
                                    Reinforcement Learning</a>
                                <br> <i>with D. Kane, S. Lui, S. Lovett
                                </i>
                                <br> <a href="https://arxiv.org/abs/2202.05444">arxiv</a> | <a
                                    href="https://twitter.com/gauravmahajn/status/1493552416943312902">tweet</a>
                            </p>
                            <p></p>
                        </li>
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/2201.03806">Learning What To Remember</a>
                                <br> <i>with R. Bhattacharjee</i>
                                <br> <a href="https://arxiv.org/abs/2201.03806">arxiv</a>
                            </p>
                            <p></p>
                        </li>
                        <li>
                            <p>
                                <a href="https://arxiv.org/abs/2111.04746">Realizable Learning is All You Need</a>
                                <br> <i>with M. Hopkins, D. Kane, S. Lovett
                                </i>
                                <br> <a href="https://arxiv.org/abs/2111.04746">arxiv</a> | <a
                                    href="https://twitter.com/thegautamkamath/status/1458466857229373447">tweet</a>
                            </p>
                            <p></p>
                        </li>
                    </ol>
                    </p>
                </div>
            </div> -->
            <div class="margin30">
                <h1>Professional Services</h1>
                <ol>
                    <li>
                        Reviewer: ICLR (2022); WORLT (2021); Neurips (2021); ICML (2021, 2020), JMLR (2022, 2021,
                        2020)
                    </li>
                </ol>
                <br>
                <br>
                <br>
                <p id="footnote" style="padding: 20px 0px 50px 0px;"> &ast; Authors are listed in alphabetical
                    order,
                    following the convention in mathematics
                    and theoretical computer science.</p>
            </div>
        </article>
    </main>
</body>

</html>