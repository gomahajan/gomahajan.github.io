<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Gaurav Mahajan</title>
    <link>http://cseweb.ucsd.edu/~gmahajan/post/</link>
    <description>Recent content in Posts on Gaurav Mahajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Gaurav Mahajan</copyright>
    <lastBuildDate>Tue, 01 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/~gmahajan/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Human Activity Recognition using Convolution Neural Network</title>
      <link>http://cseweb.ucsd.edu/~gmahajan/post/cnn-har-actitracker/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cseweb.ucsd.edu/~gmahajan/post/cnn-har-actitracker/</guid>
      <description>&lt;p&gt;In this post&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, we will discuss how to build a architecture for Human Activity Recognition using Convolution Neural Network. &lt;a href=&#34;https://github.com/gomahajan/har-actitracker&#34;&gt;Link to code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;python-libraries-used&#34;&gt;Python Libraries Used&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;TensorFlow&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;Numpy&lt;/li&gt;
&lt;li&gt;Scipy&lt;/li&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;

&lt;p&gt;We are going to build a architecture which recognizes human activities like walking, lying down, standing, sitting, jogging etc. based on accelerometer data. We will use the Actitracker dataset from &lt;a href=&#34;http://www.cis.fordham.edu/wisdm/dataset.php&#34;&gt;Wireless Sensor Data Mining Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Data for some activities in the dataset:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/har-cnn-actitracker/walking.png&#34; alt=&#34;&#34;&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/har-cnn-actitracker/lying-down.png&#34; alt=&#34;&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;

&lt;p&gt;Our architecture consists of multiple Depthwise Convolution Neural Networks (D-CONV), Max POOL and RELU layers, followed by fully connected neural network (FC) with a softmax classifier to get the class probabilities.&lt;/p&gt;

&lt;p&gt;[D-CONV -&amp;gt; RELU] -&amp;gt; POOL -&amp;gt; [D-CONV -&amp;gt; RELU] -&amp;gt; FC&lt;/p&gt;

&lt;p&gt;If you haven&#39;t heard of Depthwise Convolution Neural Networks, they are similar to normal Convolution Neural Networks except the filters are applied to each channel separately. So, a Convolution Neural Network which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, out_channels] will output a tensor of shape [batch, out_height, out_width, out_channels]. A Depthwise Convolution Neural Network which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] will output a tensor of shape [batch, out_height, out_width, in_channels*channel_multiplier] as it will separately apply channel_multiplier number of filters to each in_channel.&lt;/p&gt;

&lt;h1 id=&#34;building-the-architecture-in-tensorflow&#34;&gt;Building the architecture in TensorFlow&lt;/h1&gt;

&lt;p&gt;We create placeholder variables for our input and output values. In TensorFlow, using &#39;None&#39; as a dimension allows the variable to take any number of rows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, in_channels])
self.Y = tf.placeholder(tf.float32, shape=[None, num_labels])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we create a Depthwise Convolution Neural Network (D-CNN) which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] and will output a tensor of shape [batch, out_height, out_width, in_channels*channel_multiplier].&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filter = weight_variable([1, filter_width, in_channels, channels_multiplier])
depthwise_conv2d = tf.nn.depthwise_conv2d(input, filter, [1, 1, 1, 1], padding=&#39;VALID&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We follow the D-CNN with a RELU layer and add a bias for each channel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;biases = bias_variable([channels_multiplier * in_channels])
tf.nn.relu(tf.add(depthwise_conv2d, biases))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then add a max POOL layer which performs the max pooling over the window of ksize=[1, 1, kernel_size, 1] and uses a stride of [1, 1, stride_size, 1].
Note: We are performing the max pooling and stride only in the dimension of input_width. We do not want to pool across the channels or batch entries as we want to keep them independent. Also, input_height is 1 for D-CNN.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1],
                          strides=[1, 1, stride_size, 1], padding=&#39;VALID&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then add another D-CNN with filter tensor of shape [1, filter_width_2, in_channels_2, channel_multiplier_2]. in_channels_2 is equal to channels_multiplier * in_channels since D-CNN multiplies the number of channels by the channels_multiplier and our pooling preserved the
channel size.
Note: We, however, have to be careful when choosing the filter_width_2 as it should not be more than POOL layer&#39;s output width.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assert filter_width_2 &amp;lt;= p.shape[2], &amp;quot;Filter width 2 should be less than input width 2&amp;quot;
c = apply_depthwise_conv(p, filter_width_2, channels_multiplier * in_channels, channels_multiplier_2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We flatten the output from the second D-CNN and pass it into a neural network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shape = c.get_shape().as_list()
c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])

assert shape[3] == channels_multiplier * in_channels * channels_multiplier_2

f_weights_l1 = weight_variable([shape[1] * shape[2] * shape[3], num_hidden])
f_biases_l1 = bias_variable([num_hidden])
f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1), f_biases_l1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then add a softmax classifier which outputs the class probabilities and loss.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;out_weights = weight_variable([num_hidden, num_labels])
out_biases = bias_variable([num_labels])
self.y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)
self.loss = -tf.reduce_sum(self.Y * tf.log(self.y_))
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;preprocessing-data&#34;&gt;Preprocessing data&lt;/h1&gt;

&lt;p&gt;Using pandas library, we can read the data from the WISDM dataset using column names as indices for the returned DataFrame.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;column_names = [&#39;user-id&#39;, &#39;activity&#39;, &#39;timestamp&#39;, &#39;x-axis&#39;, &#39;y-axis&#39;, &#39;z-axis&#39;]
data = pd.read_csv(file_path, header=None, names=column_names, comment=&#39;;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then drop the rows which have &#39;nan&#39; entries and reduce the size in case we want to use only a certain percentage of the dataset &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = data.dropna(axis=0, how=&#39;any&#39;)
data = data[0:(usage * data.shape[0]) // 100]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We normalize the data independently from accelerometer along the x-axis, y-axis and z-axis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normalize(dataset):
   mu = np.mean(dataset, axis=0)
   sigma = np.std(dataset, axis=0)
   return (dataset - mu) / sigma

dataset[&#39;x-axis&#39;] = math.normalize(dataset[&#39;x-axis&#39;])
dataset[&#39;y-axis&#39;] = math.normalize(dataset[&#39;y-axis&#39;])
dataset[&#39;z-axis&#39;] = math.normalize(dataset[&#39;z-axis&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we segment the data into [windows_size, in_channels] blocks and we choose the label as the most occurring label in the window.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def create_segments(data, window_size=90):
    segments = np.empty((0, window_size, 3))
    labels = np.empty((0))
    for (start, end) in windows(data[&#39;timestamp&#39;], window_size):
        x = data[&amp;quot;x-axis&amp;quot;][start:end]
        y = data[&amp;quot;y-axis&amp;quot;][start:end]
        z = data[&amp;quot;z-axis&amp;quot;][start:end]
        if (len(data[&#39;timestamp&#39;][start:end]) == window_size):
            segments = np.vstack([segments, np.dstack([x, y, z])])
            labels = np.append(labels, stats.mode(data[&amp;quot;activity&amp;quot;][start:end])[0][0])
    return segments, labels

segments, labels = util.create_segments(dataset)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then one hot encode the labels and reshape the segments into [1, windows_size, in_channels] blocks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)
reshaped_segments = segments.reshape(len(segments), 1, 90, 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We do a 70:30 split of the data into train:test sets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_test_split = np.random.rand(len(reshaped_segments)) &amp;lt; 0.70
train_x = reshaped_segments[train_test_split]
train_y = labels[train_test_split]
test_x = reshaped_segments[~train_test_split]
test_y = labels[~train_test_split]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;training-and-evaluating-the-model&#34;&gt;Training and evaluating the model&lt;/h1&gt;

&lt;p&gt;We batch the training data into sets of 10 and use gradient descent for learning weights.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;learning_rate = 0.0001
training_epochs = 8
batch_size = 10
total_batches = train_x.shape[0] // batch_size
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)

correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

cost_history = np.empty(shape=[1], dtype=float)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We create a TensorFlow session, initialize the variables and train our model.
We print the accuracy on the training set for each epoch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with tf.Session() as session:
    tf.initialize_all_variables().run()
    for epoch in range(training_epochs):
        for b in range(total_batches):
            offset = (b * batch_size) % (train_y.shape[0] - batch_size)
            batch_x = train_x[offset:(offset + batch_size), :, :, :]
            batch_y = train_y[offset:(offset + batch_size), :]
            _, loss_value = session.run([optimizer, self.loss], feed_dict={self.X: batch_x, self.Y: batch_y})
            cost_history = np.append(cost_history, loss_value)
        print(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; Training Loss: &amp;quot;, loss_value, &amp;quot; Training Accuracy: &amp;quot;,
              session.run(accuracy, feed_dict={self.X: train_x, self.Y: train_y}))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then get our accuracy on the test set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Testing Accuracy:&amp;quot;, session.run(accuracy, feed_dict={self.X: test_x, self.Y: test_y}))
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Inspired from a similar &lt;a href=&#34;https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN&#34;&gt;post&lt;/a&gt; by Aaqib Saeed
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Helps when you are testing your architecture and do not want the &amp;quot;immenseness&amp;quot; of your data to slow you.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Finding Regression Function</title>
      <link>http://cseweb.ucsd.edu/~gmahajan/post/finding-regression-function/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cseweb.ucsd.edu/~gmahajan/post/finding-regression-function/</guid>
      <description>&lt;p&gt;In this post, we will discuss how the regression function for squared error loss function is &lt;span  class=&#34;math&#34;&gt;\(f(x)=E(Y|X=x)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Let
\(X\in R^P\) denote a real valued random input vector, and \(Y\in R\) a real valued random output variable, with joint distribution \(Pr(X,Y)\).
We seek a function \(f(X)\) for predicting \(Y\) given values of the input \(X\). This theory requires a &lt;em&gt;loss function&lt;/em&gt; \(L(Y,f(X))\) for penalizing errors in prediction, and we will use &lt;em&gt;squared error loss&lt;/em&gt;: \(L(Y,f(X))=(Y-f(X))^2\).&lt;/p&gt;

&lt;p&gt;This leads us to a criterion for choosing \(f\),&lt;/p&gt;

&lt;p&gt;\begin{align*}
EPE(f)&amp;amp; = E(Y-f(X))^2\\&lt;br&gt;
&amp;amp; = \int(y-f(x))^2 Pr(dx,dy)
\end{align*}&lt;/p&gt;

&lt;p&gt;the expected (squared) prediction error. By conditioning on \(X\), we can write \(EPE\) as &lt;span  class=&#34;math&#34;&gt;\(EPE(f) = E_X E_{Y|X} ((Y-f(X))^2|X)\)&lt;/span&gt;
and we see that it suffices to minimize \(EPE\) pointwise:
\begin{equation}
\label{epe}
f(x)=\underset{c}{\operatorname{argmin}} E_{Y|X} ([Y-c]^2|X=x)
\end{equation}&lt;/p&gt;

&lt;p&gt;For any guess t by writing \(\mu = E(Y)\), we have \(EPE(f)\) as
\begin{align*}
E[Y-t]^2 &amp;amp; = E[Y-\mu + \mu-t]^2\\&lt;br&gt;
&amp;amp; = E[Y-\mu]^2 + E[(Y-\mu)(\mu-t)] + E[\mu-t]^2\\&lt;br&gt;
&amp;amp; \geq E[Y-\mu]^2
\end{align*}&lt;/p&gt;

&lt;p&gt;Applied to conditional distribution of \(Y\) given \(X=x\), this gives us that &lt;span  class=&#34;math&#34;&gt;\(E_{Y|X}([Y-E(Y|X=x)]^2|X=x) \leq E_{Y|X} ([Y-c]^2|X=x)\)&lt;/span&gt; for all estimates \(c\).&lt;/p&gt;

&lt;p&gt;Hence, proved that the solution for equation \eqref{epe} above is &lt;span  class=&#34;math&#34;&gt;\(f(x)=E(Y|X=x)\)&lt;/span&gt;
the conditional expectation, also known as the &lt;em&gt;regression&lt;/em&gt; function.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>First Few Weeks in PhD</title>
      <link>http://cseweb.ucsd.edu/~gmahajan/post/first-few-weeks-in-phd/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cseweb.ucsd.edu/~gmahajan/post/first-few-weeks-in-phd/</guid>
      <description>&lt;p&gt;This post discusses the few tips recommended by people for first few weeks in PhD.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;formulate-what-you-plan-to-work-on&#34;&gt;Formulate what you plan to work on&lt;/h2&gt;

&lt;p&gt;Vaguely identify in few lines the problem you are trying to solve and the research area that you plan to work/publish in.&lt;/p&gt;

&lt;h2 id=&#34;list-courses-top-1020-research-papers&#34;&gt;List courses; top 10-20 research papers&lt;/h2&gt;

&lt;p&gt;List the courses that you think are relevant to your research area. Also, list top 10-20 research papers according to how relevant they are to your research area first and how many citations they have second. Discuss with your professor on what good papers and courses are missing.&lt;/p&gt;

&lt;h2 id=&#34;maintain-a-diary&#34;&gt;Maintain a Diary&lt;/h2&gt;

&lt;p&gt;Keep a log of what you do and find daily. Make notes at multiple levels of granularity  (e.g., one sentence summary of the entire paper all the way down to sentence-level notes) from papers you read.&lt;/p&gt;

&lt;h2 id=&#34;start-a-literature-review&#34;&gt;Start a Literature Review&lt;/h2&gt;

&lt;p&gt;Literature Review justifies the reason for your research and allows you to establish your theoretical framework. It will also help organize the learnings from the papers.&lt;/p&gt;

&lt;h2 id=&#34;check-a-previous-thesis-of-the-group&#34;&gt;Check a previous thesis of the group&lt;/h2&gt;

&lt;p&gt;Get an idea of who did what in your group. Learn how a PhD thesis is structured in your group.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>From Least Squares to Nearest Neighbors</title>
      <link>http://cseweb.ucsd.edu/~gmahajan/post/least-squares-and-nearest-neighbors/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cseweb.ucsd.edu/~gmahajan/post/least-squares-and-nearest-neighbors/</guid>
      <description>&lt;p&gt;In this post, we will evaluate two simple but powerful prediction methods: the linear model fit by least squares and the \(k\)-nearest-neighbor prediction rule on a simple regression problem.&lt;br&gt;
&lt;a href=&#34;https://github.com/gmahajanml/linear-to-nearest&#34;&gt;Link to code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;We will see how the linear model makes huge assumptions about structure and yields stable but possibly inaccurate predictions. The method of \(k\)-nearest neighbors, however, makes very mild structural assumptions: its predictions are often accurate but can be unstable.&lt;/p&gt;

&lt;h2 id=&#34;training-data&#34;&gt;Training Data&lt;/h2&gt;

&lt;p&gt;Consider the two possible scenarios:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 1:&lt;/strong&gt; The training data in each class were generated from bivariate Gaussian distribution with uncorrelated components and different means.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scenario 2:&lt;/strong&gt; The training data in each class came from a mixture of 10 low-variance Gaussian distributions, with individual means themselves distributed as Gaussian.&lt;/p&gt;

&lt;p&gt;We first create training data for \(Y=1\), denoted by \(X_{pos}\) and
\(Y=-1\), denoted by \(X_{neg}\).&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/linear_nearest/scenario1.png&#34; alt=&#34;&#34; title=&#34;Scenario 1&#34;&gt;&lt;figcaption&gt;Scenario 1&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/linear_nearest/scenario2.png&#34; alt=&#34;&#34; title=&#34;Scenario 2&#34;&gt;&lt;figcaption&gt;Scenario 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;results-for-least-squares&#34;&gt;Results for Least Squares&lt;/h2&gt;

&lt;p&gt;This is how least squares performs on both scenarios.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/linear_nearest/scenario1_linear.png&#34; alt=&#34;&#34; title=&#34;Least squares on Scenario 1&#34;&gt;&lt;figcaption&gt;Least squares on Scenario 1&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/linear_nearest/scenario2_linear.png&#34; alt=&#34;&#34; title=&#34;Least squares on Scenario 2&#34;&gt;&lt;figcaption&gt;Least squares on Scenario 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;results-for-nearest-neighbors&#34;&gt;Results for Nearest Neighbors&lt;/h2&gt;

&lt;p&gt;This is how nearest neighbors performs for k=10.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/linear_nearest/scenario1_nearest.png&#34; alt=&#34;&#34; title=&#34;Nearest Neighbors on Scenario 1&#34;&gt;&lt;figcaption&gt;Nearest Neighbors on Scenario 1&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://cseweb.ucsd.edu/~gmahajan/img/linear_nearest/scenario2_nearest.png&#34; alt=&#34;&#34; title=&#34;Nearest Neighbors on Scenario 2&#34;&gt;&lt;figcaption&gt;Nearest Neighbors on Scenario 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We can actually develop some theory that provides a framework for developing models such as those discussed informally so far. As discussed in another post, for squared error loss function, the regression function is &lt;span  class=&#34;math&#34;&gt;\(f(x)=E(Y|X=x)\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The nearest-neighbor methods attempt to directly implement this recipe using the training data. At each point \(x\), we might ask for the average of all those \(y_is\) with input \(x_i=x\). Since there is typically at most one observation at any point \(x\), we settle for
&lt;span  class=&#34;math&#34;&gt;\(\hat{f}(x)=Ave(y_i|x_i \in N_k(x))\)&lt;/span&gt;
where \(&amp;quot;Ave&amp;quot;\) denotes average, and \(N_k(x)\) is the neighborhood
containing the \(k\) points in \(T\) closest to \(x\). Two approximations are happening here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;expectation is approximated by averaging over sample data;&lt;/li&gt;
&lt;li&gt;conditioning at a point is related to conditioning on some region &amp;quot;close&amp;quot; to the target point&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How does linear regression fit into this framework? The simplest explanation is that one assumes that the regression function \(f(x)\) is approximately linear in its arguments: &lt;span  class=&#34;math&#34;&gt;\(f(x) \approx x^T\beta\)&lt;/span&gt;.
This is a model-based approach- we specify a model for the regression function. Plugging this linear model for \(f(x)\) into \(EPE\) and differentiating we can solve for \(\beta\) theoretically:
&lt;span  class=&#34;math&#34;&gt;\(\beta = (E(XX^T))^{-1} E(XY)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Note we have not conditioned on \(X\); rather we have used our knowledge of the functional relationship to &lt;em&gt;pool&lt;/em&gt; over values of \(X\). The least squares solution amounts to replacing the expectation b averages overs the training data.&lt;/p&gt;

&lt;p&gt;So both \(k\)-nearest neighbors and least squares end up approximating
conditional expectations by averages. But they differ dramatically in terms
of model assumptions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Least squares assumes f(x) is well approximated by a globally linear
function. The linear decision boundary from least squares is very smooth, and apparently
stable to fit. More suitable for scenario 1.&lt;/li&gt;
&lt;li&gt;k-nearest neighbors assumes f(x) is well approximated by a locally constant function. Thus, it is wiggly and unstable. It however
does not rely on any stringent assumptions about the underlying data, and can adapt
to any situation. More suitable for scenario 2.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
