<!DOCTYPE html>
<html lang="en">

<head>
  <title>ICTS: Reinforcement Learning Bootcamp</title>
  <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="../css/style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
  <article>
    <h1 style="text-align: center;margin-top: 30px;margin-bottom: 30px;"> ICTS: Reinforcement Learning Bootcamp</h1>
    <dl style="margin-bottom: 50px;">
      <dd style="margin-bottom: 10px;"><b>Lecturer</b>: <a href="../../index.html">Gaurav Mahajan</a>
        (gaurav.mahajan@yale.edu)</dd>
      <dd style="margin-bottom: 10px;"><b>Zoom Link</b>: Coming soon</dd>
      <dd style="margin-bottom: 10px;"><b>Lecture Notes</b>: <a
          href="https://www.overleaf.com/project/681ece2f97c6b6fc5f656a5d">pdf</a> </dd>
    </dl>
    <hr>
    <h3>Description</h3>
    <div style="
    margin-top: 10px;
    margin-bottom: 50px;
    margin-inline-start: 40px;
    margin-inline-end: 150px;
">The course will cover the basics of reinforcement learning theory. We will start by implementing simple gradient-based algorithms in PyTorch and using them to solve standard control problems like CartPole and the Atari 2600 game Pong. Along the way, we will explore how to optimize both the sample complexity (the number of interactions with the environment) and the computational complexity (GPU hours) needed to learn an optimal policy.
    </div>
    <hr>


    <h3 style="margin-bottom: 0.5em;">Lectures</h3>
    <div style="margin-top: 0;">
      <p style="margin: 0.8em 0 0.2em 3em;"><strong>Day 1: Basics of Reinforcement Learning</strong></p>
      <ul style="list-style-type: circle; margin: 0 0 0.5em 4em; padding-left: 0;">
        <li><em>Exploration vs Exploitation, and Credit Assignment</em></li>
        <li><em>Markov Decision Process, Value Functions</em></li>
      </ul>

      <p style="margin: 0.8em 0 0.2em 3em;"><strong>Day 2: Policy Gradient Methods</strong></p>
      <ul style="list-style-type: circle; margin: 0 0 0.5em 4em; padding-left: 0;">
        <li><em>Environments: CartPole and Pong</em></li>
        <li><em>Vanilla Policy Gradient Algorithm</em></li>
        <li><em>Implementing in Python</em></li>
      </ul>

      <p style="margin: 0.8em 0 0.2em 3em;"><strong>Day 3: Optimism and Bellman Equations</strong></p>
      <ul style="list-style-type: circle; margin: 0 0 0.5em 4em; padding-left: 0;">
        <li><em>Bellman Equation</em></li>
        <li><em>Algorithm</em></li>
        <li><em>Optimization Constraint in Linear Form</em></li>
        <li><em>Optimism: Bounding Regret</em></li>
        <li><em>Exploration: Bounding the Number of Rounds</em></li>
      </ul>

      <p style="margin: 0.8em 0 0.2em 3em;"><strong>Day 4: Computational Complexity</strong></p>
      <ul style="list-style-type: circle; margin: 0 0 0.5em 4em; padding-left: 0;">
        <li><em>Complexity Problems</em></li>
        <li><em>Linear Infinite-Horizon MDP</em></li>
        <li><em>Linear Finite-Horizon MDP</em></li>
      </ul>
    </div>

    <br>
    <br>
    <br>
    


  </article>
</body>

</html>